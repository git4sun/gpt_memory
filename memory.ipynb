{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database utils helping store history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, T\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_database(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''CREATE TABLE IF NOT EXISTS mem\n",
    "                   (id INTEGER PRIMARY KEY, text TEXT, role TEXT, ts TEXT, categories TEXT, labels TEXT,\n",
    "                   embedding REAL, continued INTEGER, level1 TEXT, level2 TEXT, level3 TEXT, user_id Text)''')\n",
    "    conn.commit()\n",
    "    return conn, cur\n",
    "\n",
    "def insert_mem(cur, data):\n",
    "    cur.execute('''INSERT INTO mem (text, role, ts, categories, labels, embedding, continued, level1, level2, level3, user_id)\n",
    "                   VALUES (:text, :role, :ts, :categories, :labels, :embedding, :continued, :level1, :level2, :level3, :user_id)''', data)\n",
    "\n",
    "\n",
    "def read_mems(cur):\n",
    "    cur.execute(\"SELECT * FROM mem\")\n",
    "    return cur.fetchall()\n",
    "\n",
    "def update_mem(cur, id, data):\n",
    "    data['id'] = id  # Ensure the ID is included in the data dictionary for the WHERE clause\n",
    "    cur.execute('''UPDATE mem SET text=:text, role=:role, ts=:ts, categories=:categories, labels=:labels,\n",
    "                   embedding=:embedding, continued=:continued, level1=:level1, level2=:level2, level3=:level3, user_id=:user_id\n",
    "                   WHERE id=:id''', data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main flow\n",
    "conn, cur = setup_database('memory.db')\n",
    "\n",
    "data = {\n",
    "    'text': 'Sample text',\n",
    "    'role': 'admin',\n",
    "    'ts': '2023-01-01 12:00:00',\n",
    "    'categories': 'category1,category2',\n",
    "    'labels': 'label1,label2',\n",
    "    'embedding': 0.123,\n",
    "    'continued': 1,\n",
    "    'level1': 'L1',\n",
    "    'level2': 'L2',\n",
    "    'level3': 'L3'\n",
    "}\n",
    "\n",
    "insert_mem(cur, data)\n",
    "conn.commit()\n",
    "\n",
    "# Closing cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Plan\n",
    "1. Given a message, find related history, \n",
    "    1. ask GPT to label if it is followup from previous questions. \n",
    "    2. score previous history for embedding distance\n",
    "    3. score history for keyword relevance \n",
    "    4. based on scores and time, calculate decay factor\n",
    "    5. summarize to fixed length given decay factor\n",
    "    6. given relevance and keyword position, implement a gradient weighting summary for the message. keyword matching will be the focus and environment words are with decaying relevance scores.  \n",
    "2. formulate the prompt based on history and scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cur = setup_database('memory.db')\n",
    "read_mems(cur)\n",
    "\n",
    "\n",
    "\n",
    "def connect_db(db_path='memory.db'):\n",
    "    \"\"\"Connect to the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def get_history(conn, user_id, limit=100):\n",
    "    \"\"\"Retrieve the message history for a given user.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT * FROM mem WHERE user_id = ? ORDER BY ts DESC LIMIT ?\"\n",
    "    cursor.execute(query, (user_id, limit))\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def update_message_info(conn, message_id, **kwargs):\n",
    "    \"\"\"Update information for a given message.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    columns = ', '.join([f\"{k} = ?\" for k in kwargs.keys()])\n",
    "    values = list(kwargs.values()) + [message_id]\n",
    "    query = f\"UPDATE mem SET {columns} WHERE id = ?\"\n",
    "    cursor.execute(query, values)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_followup_messages(conn, history):\n",
    "    \"\"\"\n",
    "    Label each message in the history as a followup or not.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: Database connection object.\n",
    "    - history: A list of history messages fetched from the database.\n",
    "               Each message is expected to be a tuple or list in the format:\n",
    "               (id, text, role, ts, categories, labels, embedding, continued, level1, level2, level3)\n",
    "\n",
    "    This function iterates through the history, uses GPT to determine if each message is a followup,\n",
    "    and updates the `continued` field in the database accordingly.\n",
    "    \"\"\"\n",
    "    # Assuming the GPT interaction function is named `ask_gpt_if_followup`\n",
    "    # and returns a boolean indicating if the message is a followup.\n",
    "    \n",
    "    for i, current_message in enumerate(history[:-1]):  # Skip the last message since there's no next message to compare with\n",
    "        current_message_id, current_text, *_ = current_message\n",
    "        next_message = history[i + 1]\n",
    "        next_message_text = next_message[1]  # Assuming the text is the second field\n",
    "        \n",
    "        # Generate the prompt for GPT\n",
    "        prompt = f\"Is the following message a followup to the previous one?\\n\\nPrevious message: {next_message_text}\\nCurrent message: {current_text}\"\n",
    "        \n",
    "        # Mock GPT interaction\n",
    "        # In a real scenario, replace this with an actual GPT call, e.g., through OpenAI's API.\n",
    "        is_followup = ask_gpt_if_followup(prompt)\n",
    "        \n",
    "        # Update the database with the result\n",
    "        # The `continued` column is updated with 1 if it's a followup, otherwise 0\n",
    "        update_message_info(conn, current_message_id, continued=int(is_followup))\n",
    "\n",
    "\n",
    "def setup_openai_api():\n",
    "    \"\"\"Configure OpenAI API key.\"\"\"\n",
    "    # Replace 'your_api_key_here' with your actual OpenAI API key\n",
    "    openai.api_key = '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'\n",
    "\n",
    "\n",
    "def ask_gpt_if_followup(previous_message, current_message):\n",
    "    \"\"\"\n",
    "    Use OpenAI's GPT-4 to determine if the current message is a followup to the previous one.\n",
    "\n",
    "    Parameters:\n",
    "    - previous_message: String, the content of the previous message in the conversation.\n",
    "    - current_message: String, the content of the current message.\n",
    "\n",
    "    Returns:\n",
    "    - True if GPT-4 determines the message is a followup, False otherwise.\n",
    "    \"\"\"\n",
    "    setup_openai_api()  # Ensure API key is set\n",
    "    \n",
    "    # Formulate the prompt for GPT-4\n",
    "    prompt = f\"Given the previous message: '{previous_message}', is the following message a follow-up? '{current_message}' Please answer 'yes' or 'no'.\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You need to determine if the current message is a follow-up to the previous one.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Interpret GPT-4's response to extract a yes or no answer\n",
    "        response_text = response.choices[0].message['content'].strip().lower()\n",
    "        if \"yes\" in response_text:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return False  # Proper error handling should be considered in a real implementation\n",
    "\n",
    "\n",
    "def get_message_embedding(message):\n",
    "    \"\"\"\n",
    "    Request GPT embedding API to get embeddings for the input message.\n",
    "\n",
    "    Parameters:\n",
    "    - message: String, the text message for which to get the embedding.\n",
    "\n",
    "    Returns:\n",
    "    - An embedding vector for the message.\n",
    "    \"\"\"\n",
    "    setup_openai_api()  # Ensure API key is set\n",
    "\n",
    "    try:\n",
    "        # Request the embedding for the message using the \"text-embedding-ada-002\" model or a similar one\n",
    "        response = openai.Embedding.create(\n",
    "            input=message,\n",
    "            model=\"text-embedding-ada-002\"  # Choose an appropriate model for your use case\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vector from the response\n",
    "        embedding = response['data'][0]['embedding']\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI Embedding API: {e}\")\n",
    "        return None  # Consider how you want to handle errors gracefully\n",
    "\n",
    "\n",
    "\n",
    "def update_message_info(conn, message_id, **kwargs):\n",
    "    \"\"\"\n",
    "    Update information for a given message in the 'mem' table of the 'memory' database.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    columns = ', '.join([f\"{k} = ?\" for k in kwargs.keys()])\n",
    "    values = list(kwargs.values()) + [message_id]\n",
    "    query = f\"UPDATE mem SET {columns} WHERE id = ?\"\n",
    "    cursor.execute(query, values)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def calculate_embedding_distance(history):\n",
    "    \"\"\"Calculate the embedding distance between messages.\"\"\"\n",
    "    # Use cosine similarity or another distance metric for embeddings.\n",
    "    pass\n",
    "\n",
    "def calculate_keyword_relevance(history, keywords):\n",
    "    \"\"\"Score messages based on keyword relevance.\"\"\"\n",
    "    # This could involve NLP techniques to find keyword occurrences and relevance.\n",
    "    pass\n",
    "\n",
    "def calculate_decay_factor(history, time_now):\n",
    "    \"\"\"Compute a decay factor for each message based on scores and time.\"\"\"\n",
    "    # Decay factor calculation based on time and other scores.\n",
    "    pass\n",
    "\n",
    "def summarize_history(history, decay_factors):\n",
    "    \"\"\"Summarize the message history into a fixed length.\"\"\"\n",
    "    # Summarization logic using decay factors to weigh message importance.\n",
    "    pass\n",
    "\n",
    "def calculate_distance(embedding1, embedding2):\n",
    "    \"\"\"Calculate the cosine similarity between two embeddings.\"\"\"\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "def formulate_gpt_prompt(history):\n",
    "    \"\"\"Formulate the GPT prompt based on processed history and scores.\"\"\"\n",
    "    # Concatenate and format the history and scores into a GPT prompt.\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chat! Type your message and hit enter. Type 'stop' or press Ctrl-D to end the conversation.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_memory",
   "language": "python",
   "name": "gpt_memory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
